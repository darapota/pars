#Хеллоу, дорогой товарищ. Тебе тоже нравится Андрей Александрович?
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syRbvmJbl5_K"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Функция для сбора ссылок с одной страницы\n",
        "def get_links_from_page(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Находим все теги <a> с классом \"news__title\" и извлекаем из них атрибут href\n",
        "    links = [a['href'] for a in soup.select('.news__title a')]\n",
        "\n",
        "    return links\n",
        "\n",
        "# URL страницы новостей\n",
        "news_page_url = 'https://news.tpu.ru/news/archive'\n",
        "\n",
        "# Переменная для хранения всех собранных ссылок\n",
        "all_links = []\n",
        "\n",
        "# Цикл для сбора ссылок с нескольких страниц\n",
        "for page_number in range(3, 701):  # например, собираем ссылки с первых пяти страниц\n",
        "    page_url = f'{news_page_url}/?PAGEN_3={page_number}'\n",
        "    links_on_page = get_links_from_page(page_url)\n",
        "    all_links.extend(links_on_page)\n",
        "\n",
        "# Сохраняем ссылки в текстовый файл\n",
        "with open('links.txt', 'w', encoding='utf-8') as file:\n",
        "    for link in all_links:\n",
        "        file.write(link + '\\n')\n",
        "# Функция для сбора данных\n",
        "def get_data_from_page(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Извлекаем данные\n",
        "    title_element = soup.select_one('.page__title')\n",
        "    title_text = title_element.text.strip() if title_element else ''\n",
        "\n",
        "    tags = [tag.text.strip() for tag in soup.select('.page-tags__item')]\n",
        "\n",
        "    date_element = soup.select_one('.page-post-info__date')\n",
        "    date = date_element.text.strip() if date_element else ''\n",
        "\n",
        "    time_element = soup.select_one('.page-post-info__time')\n",
        "    time = time_element.text.strip() if time_element else ''\n",
        "\n",
        "    description_element = soup.select_one('.page__description')\n",
        "    description = description_element.text.strip() if description_element else ''\n",
        "\n",
        "    lead_element = soup.select_one('.page-post-info__views')\n",
        "    lead = lead_element.text.strip() if lead_element else ''\n",
        "\n",
        "    text_elements = soup.select('.text-content p')\n",
        "    text = ' '.join([paragraph.text.strip() for paragraph in text_elements])\n",
        "\n",
        "    return {\n",
        "        'Заголовок': title_text,\n",
        "        'Теги': ', '.join(tags) if tags else '',\n",
        "        'Дата': date,\n",
        "        'Время': time,\n",
        "        'Описание': description,\n",
        "        'Лид': lead,\n",
        "        'Текст': text,\n",
        "        'Ссылка': url\n",
        "    }\n",
        "\n",
        "# Читаем ссылки из файла\n",
        "with open('links.txt', 'r') as file:\n",
        "    links = file.read().splitlines()\n",
        "\n",
        "# Цикл для обработки каждой ссылки\n",
        "for link in links:\n",
        "    try:\n",
        "        data = get_data_from_page(link)\n",
        "        print(data)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {link}: {str(e)}\")\n",
        "#Создание Датасета\n",
        "file_path = '/content/drive/MyDrive/database.txt'\n",
        "\n",
        "# Считывание данных из файла\n",
        "with open(file_path, 'r') as file:\n",
        "    data_list = [eval(line) for line in file]\n",
        "\n",
        "# Создание DataFrame\n",
        "df = pd.DataFrame(data_list)\n",
        "\n",
        "# Вывод DataFrame\n",
        "print(df)\n",
        "# Замените 'output_file.csv' на желаемое имя выходного файла\n",
        "output_file = 'output_file.csv'\n",
        "\n",
        "# Сохранение DataFrame в файл CSV\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "import numpy as np\n",
        "print(df.loc[df['Текст'].isnull()])\n",
        "final_df = df.drop(index = [])\n",
        "final_df['Текст'].isna().sum()\n",
        "words = ['университет',]\n",
        "new_df = final_df[final_df['Текст'].str.contains('|'.join(words), case = False)]\n",
        "new_df\n",
        "##Чтение датасета с текстом\n",
        "df = pd.read_csv(\"output_file.csv\", encoding='utf-8', sep=\",\")\n",
        "df.head()"
      ]
    }
  ]
}
